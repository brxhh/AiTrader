import yfinance as yf
import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
# –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Ç–µ—Å—Ç

# --- 1. –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• (BIG DATA) ---
TICKER = "GC=F"
DXY_TICKER = "DX-Y.NYB"

print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ï –¥–∞–Ω–Ω—ã–µ (15 –ª–µ—Ç) –¥–ª—è {TICKER} –∏ {DXY_TICKER}...")
df = yf.download(TICKER, period="15y", interval="1d", progress=False)
dxy = yf.download(DXY_TICKER, period="15y", interval="1d", progress=False)

if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)
if isinstance(dxy.columns, pd.MultiIndex): dxy.columns = dxy.columns.get_level_values(0)


# --- 2. –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–´–ï –§–ò–ß–ò (–ù–û–í–´–ô –ù–ê–ë–û–†) ---
def add_ultimate_features(df, dxy_df):
    df = df.copy()

    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
    df.index = df.index.tz_localize(None)
    dxy_df.index = dxy_df.index.tz_localize(None)
    dxy_aligned = dxy_df.reindex(df.index, method='ffill')
    df['Close_DXY'] = dxy_aligned['Close'].bfill()

    df['DayOfWeek'] = df.index.dayofweek
    df['Month'] = df.index.month

    df['Log_Ret'] = np.log(df['Close'] / df['Close'].shift(1))

    for lag in [1, 2, 3, 5, 10, 20]:
        df[f'Lag_{lag}'] = df['Log_Ret'].shift(lag)

    df['EMA_50'] = df['Close'].ewm(span=50, adjust=False).mean()
    df['Dist_EMA'] = (df['Close'] - df['EMA_50']) / df['EMA_50']

    ema12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = ema12 - ema26
    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['MACD_Signal']

    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rs = gain / loss
    df['RSI'] = 100 - (100 / (1 + rs))

    high_low = df['High'] - df['Low']
    true_range = np.maximum(high_low, np.abs(df['High'] - df['Close'].shift()))
    df['ATR'] = true_range.rolling(14).mean() / df['Close']

    sma20 = df['Close'].rolling(20).mean()
    std20 = df['Close'].rolling(20).std()
    df['BB_Upper'] = sma20 + 2 * std20
    df['BB_Lower'] = sma20 - 2 * std20
    df['BB_Pos'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])

    df['Body_Size'] = np.abs(df['Close'] - df['Open']) / df['Open']
    df['Shadow_Upper'] = (df['High'] - np.maximum(df['Close'], df['Open'])) / df['Open']
    df['Shadow_Lower'] = (np.minimum(df['Close'], df['Open']) - df['Low']) / df['Open']

    df['DXY_Ret'] = df['Close_DXY'].pct_change()
    df['Corr_DXY'] = df['Close'].rolling(20).corr(df['Close_DXY']).fillna(0)

    df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)

    return df.dropna()


print("üß† –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏...")
data = add_ultimate_features(df, dxy)

# –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
drop_cols = ['Target', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close', 'Close_DXY', 'BB_Upper', 'BB_Lower']
feature_names = [col for col in data.columns if col not in drop_cols]

print(f"üìä –ò—Ç–æ–≥–æ —Ñ–∏—á: {len(feature_names)}")
X = data[feature_names]
y = data['Target']

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ
split = int(len(X) * 0.85)  # 85% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, 15% –Ω–∞ —Ç–µ—Å—Ç
X_train, X_test = X.iloc[:split], X.iloc[split:]
y_train, y_test = y.iloc[:split], y.iloc[split:]

# --- 3. –ü–û–ò–°–ö –õ–£–ß–®–ò–• –ù–ê–°–¢–†–û–ï–ö (SUPER GRID) ---
print("üèãÔ∏è‚Äç‚ôÇÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –ú–ê–°–®–¢–ê–ë–ù–£–Æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É (Gradient Boosting)...")

param_dist = {
    'n_estimators': [100, 200, 300, 500],  # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
    'learning_rate': [0.005, 0.01, 0.05, 0.1],  # –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏
    'max_depth': [3, 4, 5, 7, 9],  # –†–∞–∑–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è
    'subsample': [0.7, 0.8, 0.9, 1.0],  # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model = GradientBoostingClassifier(random_state=42)

random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=50,  # 50 –ø–æ–ø—ã—Ç–æ–∫ –Ω–∞–π—Ç–∏ –∏–¥–µ–∞–ª (–±—ã–ª–æ 20)
    cv=5,  # 5 –ø—Ä–æ–≤–µ—Ä–æ–∫ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ (–±—ã–ª–æ 3)
    verbose=1,
    n_jobs=-1,
    scoring='accuracy'
)

random_search.fit(X_train, y_train)

# --- 4. –†–ï–ó–£–õ–¨–¢–ê–¢–´ ---
best_model = random_search.best_estimator_
print(f"\nüèÜ –õ–£–ß–®–ò–ï –ü–ê–†–ê–ú–ï–¢–†–´: {random_search.best_params_}")

y_pred = best_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ (–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ): {acc * 100:.2f}%")
print(classification_report(y_test, y_pred))

# --- 5. –°–û–•–†–ê–ù–ï–ù–ò–ï ---
safe_name = TICKER.replace("=", "").replace("-", "")
joblib.dump(best_model, f"robust_model_{safe_name}.pkl")
joblib.dump(feature_names, f"robust_features_{safe_name}.pkl")

print(f"‚úÖ SUPER AI —Å–æ—Ö—Ä–∞–Ω–µ–Ω! –¢–µ–ø–µ—Ä—å –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –æ–±–Ω–æ–≤–∏ app.py!")